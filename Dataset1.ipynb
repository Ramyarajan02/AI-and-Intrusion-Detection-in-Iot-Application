{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install pandas numpy scikit-learn tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zx-fPv1p93xI",
        "outputId": "03eff4df-76ab-4b61-bd33-c374c13a09cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.utils import to_categorical  # Import this\n",
        "\n",
        "\n",
        "# Load dataset (Ensure you replace 'your_dataset.csv' with the actual file)\n",
        "df = pd.read_csv(\"/content/UNSW_NB15_training-set.csv\")\n"
      ],
      "metadata": {
        "id": "-SJfadupi7z5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ML Training**"
      ],
      "metadata": {
        "id": "1kMj3Ov26Nzq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert labels to one-hot encoding\n",
        "y_train_dl = to_categorical(y_train, num_classes=len(target_encoder.classes_))\n",
        "y_test_dl = to_categorical(y_test, num_classes=len(target_encoder.classes_))\n",
        "\n",
        "# Identify categorical and numerical columns\n",
        "categorical_columns = ['proto', 'service', 'state', 'attack_cat']\n",
        "numerical_columns = [col for col in df.columns if col not in categorical_columns + ['label']]\n",
        "\n",
        "# Encode categorical features\n",
        "label_encoders = {}\n",
        "for col in categorical_columns:\n",
        "    le = LabelEncoder()\n",
        "    df[col] = le.fit_transform(df[col])\n",
        "    label_encoders[col] = le\n",
        "\n",
        "# Encode the target variable (Ensuring No Unseen Labels)\n",
        "target_encoder = LabelEncoder()\n",
        "df['label'] = target_encoder.fit_transform(df['label'])\n",
        "\n",
        "# Split the dataset into Train & Test (80-20 Split)\n",
        "X = df.drop(columns=['label'])\n",
        "y = df['label']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Handle Unseen Categories in Test Data\n",
        "for col in categorical_columns:\n",
        "    X_test[col] = X_test[col].apply(lambda x: x if x in label_encoders[col].classes_ else -1)\n",
        "\n",
        "# Normalize numerical features\n",
        "scaler = StandardScaler()\n",
        "X_train[numerical_columns] = scaler.fit_transform(X_train[numerical_columns])\n",
        "X_test[numerical_columns] = scaler.transform(X_test[numerical_columns])\n",
        "\n",
        "# Ensure y_test has only seen labels\n",
        "y_test = y_test[y_test.isin(y_train.unique())]\n",
        "X_test = X_test.loc[y_test.index]\n",
        "\n",
        "# === MACHINE LEARNING MODEL: Random Forest ===\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "\n",
        "# Evaluate ML Model\n",
        "print(f\"Random Forest Accuracy: {accuracy_score(y_test, y_pred_rf):.4f}\")\n",
        "print(\"Random Forest Classification Report:\")\n",
        "print(classification_report(y_test_filtered, y_pred_filtered, labels=list(range(len(labels))), target_names=labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycIyNX896DMg",
        "outputId": "179f2977-3793-408b-c9ec-6ba7d14fc0c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Accuracy: 0.9232\n",
            "Random Forest Classification Report:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "      Analysis       0.61      0.24      0.34       253\n",
            "      Backdoor       0.42      0.32      0.36       209\n",
            "           DoS       0.35      0.29      0.31      1361\n",
            "      Exploits       0.68      0.84      0.75      3859\n",
            "       Fuzzers       0.93      0.88      0.91      2135\n",
            "       Generic       1.00      0.98      0.99      1791\n",
            "        Normal       1.00      1.00      1.00       961\n",
            "Reconnaissance       0.88      0.67      0.76      1185\n",
            "     Shellcode       0.55      0.32      0.41       105\n",
            "         Worms       1.00      0.22      0.36        18\n",
            "\n",
            "      accuracy                           0.78     11877\n",
            "     macro avg       0.74      0.58      0.62     11877\n",
            "  weighted avg       0.77      0.78      0.77     11877\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Testing**"
      ],
      "metadata": {
        "id": "bpjmkyRE6cnU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Testing Dataset\n",
        "df_test = pd.read_csv(\"/content/testing.csv\")\n",
        "\n",
        "# Preprocess Testing Data (Same as Training)\n",
        "categorical_columns = ['proto', 'service', 'state', 'attack_cat']\n",
        "numerical_columns = [col for col in df_test.columns if col not in categorical_columns + ['label']]\n",
        "\n",
        "# Encode Categorical Features using the same LabelEncoders\n",
        "for col in categorical_columns:\n",
        "    df_test[col] = df_test[col].apply(lambda x: label_encoders[col].transform([x])[0] if x in label_encoders[col].classes_ else -1)\n",
        "\n",
        "# Standardize Numerical Features using the same Scaler\n",
        "df_test[numerical_columns] = scaler.transform(df_test[numerical_columns])\n",
        "\n",
        "# Predict using the ML Model\n",
        "y_pred_test = rf_model.predict(df_test.drop(columns=['label']))\n",
        "\n",
        "# Convert Predicted Labels back to Attack Categories\n",
        "df_test['Predicted_Label'] = y_pred_test\n",
        "df_test['Attack_Type'] = target_encoder.inverse_transform(y_pred_test)  # Convert back to attack name\n",
        "\n",
        "# Identify if Attack is Detected\n",
        "df_test['Attack_Detected'] = df_test['Predicted_Label'].apply(lambda x: 'Yes' if x > 0 else 'No')\n",
        "\n",
        "# Display Results\n",
        "print(df_test[['Attack_Detected', 'Attack_Type']])\n",
        "\n",
        "# Save Results to CSV\n",
        "df_test[['Attack_Detected', 'Attack_Type']].to_csv(\"attack_detection_results.csv\", index=False)\n",
        "print(\"Results saved as attack_detection_results.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDdBxHMat2-E",
        "outputId": "46477811-e7f1-45b0-ae04-4fb545850412"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Attack_Detected  Attack_Type\n",
            "0               No            0\n",
            "1               No            0\n",
            "2               No            0\n",
            "3               No            0\n",
            "4               No            0\n",
            "..             ...          ...\n",
            "82             Yes            1\n",
            "83             Yes            1\n",
            "84             Yes            1\n",
            "85             Yes            1\n",
            "86             Yes            1\n",
            "\n",
            "[87 rows x 2 columns]\n",
            "Results saved as attack_detection_results.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Load Testing Dataset\n",
        "df_test = pd.read_csv(\"/content/testing.csv\")\n",
        "\n",
        "# Define attack type mapping\n",
        "attack_mapping = {0: 'Normal', 1: 'Fuzzers', 2: 'Analysis', 3: 'Backdoor', 4: 'DoS',\n",
        "                  5: 'Exploits', 6: 'Generic', 7: 'Reconnaissance', 8: 'Shellcode', 9: 'Worms'}\n",
        "\n",
        "# Ensure attack_cat column exists\n",
        "if 'attack_cat' in df_test.columns:\n",
        "    df_test['Actual_Attack_Type'] = df_test['attack_cat'].map(attack_mapping)\n",
        "else:\n",
        "    raise ValueError(\"Column 'attack_cat' not found in testing dataset.\")\n",
        "\n",
        "# Simulating Model Prediction (Replace this with actual model predictions)\n",
        "np.random.seed()  # Remove fixed seed for randomness\n",
        "df_test['Predicted_Label'] = np.random.choice(list(attack_mapping.keys()), size=len(df_test))\n",
        "\n",
        "# Convert Predicted Labels to Attack Type Names\n",
        "df_test['Predicted_Attack_Type'] = df_test['Predicted_Label'].map(attack_mapping)\n",
        "\n",
        "# Identify if Attack is Detected\n",
        "df_test['Attack_Detected'] = df_test['Predicted_Attack_Type'].apply(lambda x: 'Yes' if x != \"Normal\" else 'No')\n",
        "\n",
        "# Take 5 random samples from the dataset\n",
        "random_samples = df_test.sample(n=5, random_state=None)\n",
        "\n",
        "# Display Results\n",
        "print(\"\\n=== Attack Detection Results (Random Samples) ===\")\n",
        "for index, row in random_samples.iterrows():\n",
        "    print(f\"Sample {index}:\")\n",
        "    print(f\"  Attack Detected: {row['Attack_Detected']}\")\n",
        "    print(f\"  Attack Type: {row['Predicted_Attack_Type']}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0aUPv5w25dOH",
        "outputId": "0e489bb0-a8a6-4728-bf8f-5230f894944f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Attack Detection Results (Random Samples) ===\n",
            "Sample 76:\n",
            "  Attack Detected: No\n",
            "  Attack Type: Normal\n",
            "\n",
            "Sample 11:\n",
            "  Attack Detected: Yes\n",
            "  Attack Type: Reconnaissance\n",
            "\n",
            "Sample 30:\n",
            "  Attack Detected: Yes\n",
            "  Attack Type: DoS\n",
            "\n",
            "Sample 8:\n",
            "  Attack Detected: Yes\n",
            "  Attack Type: Fuzzers\n",
            "\n",
            "Sample 82:\n",
            "  Attack Detected: No\n",
            "  Attack Type: Normal\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section\n",
        "\n"
      ],
      "metadata": {
        "id": "Q3Z0GerF6mEu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Deep Learnnig Training**"
      ],
      "metadata": {
        "id": "JKMwPCIY5zpF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# One-hot encode the labels\n",
        "y_train_dl = to_categorical(y_train, num_classes=len(target_encoder.classes_))\n",
        "y_test_dl = to_categorical(y_test, num_classes=len(target_encoder.classes_))\n",
        "\n",
        "# Define the Neural Network model\n",
        "model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    Dropout(0.3),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(len(target_encoder.classes_), activation='softmax')  # Output layer for classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train_dl, epochs=20, batch_size=32, validation_data=(X_test, y_test_dl), verbose=1)\n",
        "\n",
        "# Predict labels and convert back to class indices\n",
        "y_pred_dl = np.argmax(model.predict(X_test), axis=1)\n",
        "\n",
        "# Evaluate the model\n",
        "print(f\"Neural Network Accuracy: {accuracy_score(y_test, y_pred_dl):.4f}\")\n",
        "print(\"Neural Network Classification Report:\")\n",
        "\n",
        "# Convert target names to string\n",
        "target_names = list(map(str, target_encoder.classes_))  # Ensure labels are strings\n",
        "print(classification_report(y_test, y_pred_dl, target_names=target_names))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7yNhkAg5z5f",
        "outputId": "a44bf353-40ec-4679-cc44-b73119227a1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1485/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8780 - loss: 0.9765 - val_accuracy: 0.9110 - val_loss: 0.3713\n",
            "Epoch 2/20\n",
            "\u001b[1m1485/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.9257 - loss: 0.1620 - val_accuracy: 0.9248 - val_loss: 0.3443\n",
            "Epoch 3/20\n",
            "\u001b[1m1485/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.9530 - loss: 0.1096 - val_accuracy: 0.9250 - val_loss: 0.4711\n",
            "Epoch 4/20\n",
            "\u001b[1m1485/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9772 - loss: 0.0621 - val_accuracy: 0.9267 - val_loss: 0.6943\n",
            "Epoch 5/20\n",
            "\u001b[1m1485/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9844 - loss: 0.0459 - val_accuracy: 0.9248 - val_loss: 1.0197\n",
            "Epoch 6/20\n",
            "\u001b[1m1485/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9891 - loss: 0.0331 - val_accuracy: 0.9231 - val_loss: 1.3393\n",
            "Epoch 7/20\n",
            "\u001b[1m1485/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9894 - loss: 0.0334 - val_accuracy: 0.9231 - val_loss: 1.4461\n",
            "Epoch 8/20\n",
            "\u001b[1m1485/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9913 - loss: 0.0245 - val_accuracy: 0.9233 - val_loss: 1.5222\n",
            "Epoch 9/20\n",
            "\u001b[1m1485/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.9936 - loss: 0.0190 - val_accuracy: 0.9231 - val_loss: 1.8830\n",
            "Epoch 10/20\n",
            "\u001b[1m1485/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9938 - loss: 0.0191 - val_accuracy: 0.9230 - val_loss: 1.9681\n",
            "Epoch 11/20\n",
            "\u001b[1m1485/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 0.9940 - loss: 0.0211 - val_accuracy: 0.9232 - val_loss: 1.9328\n",
            "Epoch 12/20\n",
            "\u001b[1m1485/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.9929 - loss: 0.0198 - val_accuracy: 0.9231 - val_loss: 1.9967\n",
            "Epoch 13/20\n",
            "\u001b[1m1485/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9937 - loss: 0.0200 - val_accuracy: 0.9230 - val_loss: 2.4404\n",
            "Epoch 14/20\n",
            "\u001b[1m1485/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9931 - loss: 0.0214 - val_accuracy: 0.9230 - val_loss: 2.9535\n",
            "Epoch 15/20\n",
            "\u001b[1m1485/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9947 - loss: 0.0166 - val_accuracy: 0.9230 - val_loss: 2.9357\n",
            "Epoch 16/20\n",
            "\u001b[1m1485/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.9948 - loss: 0.0178 - val_accuracy: 0.9230 - val_loss: 3.3301\n",
            "Epoch 17/20\n",
            "\u001b[1m1485/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9934 - loss: 0.0181 - val_accuracy: 0.9230 - val_loss: 3.8082\n",
            "Epoch 18/20\n",
            "\u001b[1m1485/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9945 - loss: 0.0164 - val_accuracy: 0.9230 - val_loss: 4.2729\n",
            "Epoch 19/20\n",
            "\u001b[1m1485/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9946 - loss: 0.0166 - val_accuracy: 0.9230 - val_loss: 4.4483\n",
            "Epoch 20/20\n",
            "\u001b[1m1485/1485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9965 - loss: 0.0101 - val_accuracy: 0.9230 - val_loss: 4.7646\n",
            "\u001b[1m372/372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "Neural Network Accuracy: 0.9230\n",
            "Neural Network Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.05      0.09       961\n",
            "           1       0.92      1.00      0.96     10916\n",
            "\n",
            "    accuracy                           0.92     11877\n",
            "   macro avg       0.96      0.52      0.53     11877\n",
            "weighted avg       0.93      0.92      0.89     11877\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Testing**"
      ],
      "metadata": {
        "id": "WqnS5d6N7mwl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load test dataset\n",
        "df = pd.read_csv('/content/testing.csv')  # Updated path\n",
        "\n",
        "# Set label column\n",
        "label_column = 'attack_cat'\n",
        "\n",
        "# Check if label column exists\n",
        "if label_column not in df.columns:\n",
        "    raise ValueError(f\"Column '{label_column}' not found in dataset.\")\n",
        "\n",
        "# Separate features and labels\n",
        "X = df.drop(columns=[label_column])\n",
        "y = df[label_column]\n",
        "\n",
        "# Optional: Handle categorical columns (only if they exist)\n",
        "categorical_columns = ['protocol_type', 'flag']\n",
        "existing_categoricals = [col for col in categorical_columns if col in X.columns]\n",
        "\n",
        "if existing_categoricals:\n",
        "    X = pd.get_dummies(X, columns=existing_categoricals)\n",
        "\n",
        "# Split into train/test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Load model and encoders\n",
        "model = load_model(\"attack_detection_model.h5\")\n",
        "\n",
        "with open(\"target_encoder.pkl\", \"rb\") as f:\n",
        "    target_encoder = pickle.load(f)\n",
        "\n",
        "with open(\"attack_mapping.pkl\", \"rb\") as f:\n",
        "    attack_mapping = pickle.load(f)\n",
        "\n",
        "# Apply label encoding to y_test if needed\n",
        "y_test_encoded = target_encoder.transform(y_test)\n",
        "\n",
        "# Predict\n",
        "y_pred_dl = np.argmax(model.predict(X_test), axis=1)\n",
        "\n",
        "# Create result dataframe\n",
        "df_test = X_test.copy().reset_index(drop=True)\n",
        "df_test[label_column] = y_test.reset_index(drop=True)\n",
        "df_test['Predicted_Label'] = y_pred_dl\n",
        "df_test['Predicted_Attack_Type'] = df_test['Predicted_Label'].map(attack_mapping)\n",
        "\n",
        "# Mark detection\n",
        "df_test['Attack_Detected'] = df_test['Predicted_Attack_Type'].apply(\n",
        "    lambda x: 'Yes' if x != \"Normal\" else 'No'\n",
        ")\n",
        "\n",
        "# Show random samples\n",
        "random_samples = df_test.sample(n=5, random_state=None)\n",
        "for index, row in random_samples.iterrows():\n",
        "    print(f\"Sample {index}:\")\n",
        "    print(f\"  Attack Detected: {row['Attack_Detected']}\")\n",
        "    print(f\"  Attack Type: {row['Predicted_Attack_Type']}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "lZz3YrEDoB5L",
        "outputId": "56eaf587-c185-42e9-c4fd-0bcade567e20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] Unable to synchronously open file (unable to open file: name = 'attack_detection_model.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-077f4928c720>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# Load model and encoders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"attack_detection_model.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"target_encoder.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_api.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    194\u001b[0m         )\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".h5\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\".hdf5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         return legacy_h5_format.load_model_from_hdf5(\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/legacy/saving/legacy_h5_format.py\u001b[0m in \u001b[0;36mload_model_from_hdf5\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0mopened_new_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mopened_new_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[1;32m    562\u001b[0m                                  \u001b[0mfs_persist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs_persist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs_threshold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m                                  fs_page_size=fs_page_size)\n\u001b[0;32m--> 564\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to synchronously open file (unable to open file: name = 'attack_detection_model.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
          ]
        }
      ]
    }
  ]
}